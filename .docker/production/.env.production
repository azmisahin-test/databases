# .env.production
APP_ENV=production
NODE_ENV=${APP_ENV}

# Message Brokers
# Kafka
KAFKA_NODE_ID=1
KAFKA_LISTENER_SECURITY_PROTOCOL_MAP='CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT'
KAFKA_ADVERTISED_LISTENERS='PLAINTEXT_HOST://localhost:9092,PLAINTEXT://kafka:19092' # Internal docker communication
KAFKA_LISTENERS='CONTROLLER://:29093,PLAINTEXT_HOST://:9092,PLAINTEXT://:19092' # Internal docker communication
KAFKA_PROCESS_ROLES='broker,controller'
KAFKA_CONTROLLER_QUORUM_VOTERS='1@kafka:29093'
KAFKA_INTER_BROKER_LISTENER_NAME='PLAINTEXT'
KAFKA_CONTROLLER_LISTENER_NAMES='CONTROLLER'
CLUSTER_ID='cluster-4hrfc'
KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1
KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS=0 # Initial rebalance delay
KAFKA_SESSION_TIMEOUT_MS=1000 # consumer connection time to 1 seconds
KAFKA_TRANSACTION_STATE_LOG_MIN_ISR=1
KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR=1
KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1
KAFKA_LOG_RETENTION_HOURS=168 # Keep messages for 7 days (168 hours)
KAFKA_LOG_RETENTION_BYTES=1073741824 # Delete messages if they exceed 1GB
KAFKA_LOG_DIRS='/tmp/kraft-combined-logs'
KAFKA_NUM_PARTITIONS=3

# Kafka Environment Convert
KAFKA_BROKERS=localhost:9092

#
RABBITMQ_USER=guest
RABBITMQ_PASSWORD=guest

# rabbitmq Environment Convert
RABBITMQ_DEFAULT_USER=${RABBITMQ_USER}
RABBITMQ_DEFAULT_PASS=${RABBITMQ_PASSWORD}

# Databases
# MySQL
MYSQL_DB_HOST_NAME=mysql
MYSQL_DB_HOST_PORT=3306
MYSQL_DB_DATABASE_NAME=mydatabase
MYSQL_DB_USER=myuser
MYSQL_DB_PASSWORD=mypassword
MYSQL_DB_ROOT_PASSWORD=root_password

# MySQL Configuration
MYSQL_DB_SOCKET=/var/run/mysqld/mysqld.sock
MYSQL_DB_CHARACTER_SET=utf8mb4
MYSQL_DB_COLLATION=utf8mb4_unicode_ci

# MySQL Environment Convert
MYSQL_ROOT_PASSWORD=${MYSQL_DB_ROOT_PASSWORD}
MYSQL_DATABASE=${MYSQL_DB_DATABASE_NAME}
MYSQL_USER=${MYSQL_DB_USER}
MYSQL_PASSWORD=${MYSQL_DB_PASSWORD}

#
MONGO_INITDB_ROOT_USERNAME=mongo_user
MONGO_INITDB_ROOT_PASSWORD=mongo_password
MONGO_STORAGE_ENGINE=inMemory
MONGO_STORAGE_ENGINE=default

#
POSTGRES_DB_NAME=postgres_db
# Environment convert
POSTGRES_USER=postgres_user
POSTGRES_PASSWORD=postgres_password
POSTGRES_DB=${POSTGRES_DB_NAME}

# 
CASSANDRA_CLUSTER_NAME=my_cassandra_cluster
CASSANDRA_DC=DC1
CASSANDRA_RACK=Rack1
CASSANDRA_USER=admin
CASSANDRA_PASSWORD=admin

# ClickHouse
CLICKHOUSE_USER=default
CLICKHOUSE_PASSWORD=default

# InfluxDB
INFLUXDB_DB=my_influx_db
INFLUXDB_ADMIN_USER=influx_admin
INFLUXDB_ADMIN_PASSWORD=influx_password

# Neo4j
NEO4J_PASSWORD=neo4j_password
# Environment convert
NEO4J_AUTH=neo4j/${NEO4J_PASSWORD}

# CouchDB
COUCHDB_USER=couch_user
COUCHDB_PASSWORD=couch_password

# Monitoring
DOMAIN=mydomain.com

# mysql_ui
PMA_HOST=mysql
MYSQL_ROOT_PASSWORD=${MYSQL_ROOT_PASSWORD}

# mongo_ui
ME_CONFIG_MONGODB_ADMINUSERNAME=${MONGO_INITDB_ROOT_USERNAME}
ME_CONFIG_MONGODB_ADMINPASSWORD=${MONGO_INITDB_ROOT_PASSWORD}
ME_CONFIG_MONGODB_URL=mongodb//${MONGO_INITDB_ROOT_USERNAME}:${MONGO_INITDB_ROOT_PASSWORD}@mongo:27017/
ME_CONFIG_BASICAUTH=false

# postgresql_ui
PGADMIN_DEFAULT_EMAIL="${POSTGRES_USER}@${DOMAIN}"
PGADMIN_DEFAULT_PASSWORD="${POSTGRES_PASSWORD}"

# Elasticsearch Services
# Password for the 'elastic' user (at least 6 characters)
ELASTIC_PASSWORD=elastic
# Password for the 'kibana_system' user (at least 6 characters)
KIBANA_PASSWORD=elastic
# Version of Elastic products
# STACK_VERSION=8.16.1

# Set the cluster name
# CLUSTER_NAME=docker-cluster

# Set to 'basic' or 'trial' to automatically start the 30-day trial
LICENSE=basic
#LICENSE=trial

# Port to expose Elasticsearch HTTP API to the host
ES_PORT=9200

# Port to expose Kibana to the host
KIBANA_PORT=5601

# Increase or decrease based on the available host memory (in bytes)
ES_MEM_LIMIT=1073741824
KB_MEM_LIMIT=1073741824
LS_MEM_LIMIT=1073741824


# SAMPLE Predefined Key only to be used in POC environments
ENCRYPTION_KEY=c34d38b3a14956121ff2170e5030b471551370178f43e5626eec58b04a30fae2

# Kibana
# elasticsearch-create-enrollment-token --scope kibana
# http://localhost:5601/
# [elasticsearch].username]: value of "elastic" is forbidden
# ELASTICSEARCH_USERNAME=elastic
# ELASTICSEARCH_PASSWORD=${ELASTIC_PASSWORD}

# prometheus

# Prometheus Exporter

# mysql-exporter
DATA_SOURCE_NAME=root:${MYSQL_DB_ROOT_PASSWORD}@(mysql:3306)/
# mongo-exporter
MONGODB_URI=mongodb://mongo:27017
# postgres-exporter
DATA_SOURCE_NAME=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgresql:5432/
# kafka-exporter
KAFKA_SERVER=kafka:9093
# redis-exporter
REDIS_ADDR=redis:6379
# grafana
GF_SECURITY_ADMIN_PASSWORD=admin  # Grafana Admin password
#Graylog
GRAYLOG_PASSWORD_SECRET="somepasswordpepper"
# Password: "admin"
GRAYLOG_ROOT_PASSWORD_SHA2="8c6976e5b5410415bde908bd4dee15dfb167a9c873fc4bb8a81f6f2ab448a918"
GRAYLOG_HTTP_EXTERNAL_URI="http://127.0.0.1:9000/"
# web

# COMPOSE_PROJECT_NAME="databases"